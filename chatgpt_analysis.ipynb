{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wc/1fgpcwpx4rzfbbc7shb_wy8h0000gn/T/ipykernel_12913/2235448322.py:3: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, Markdown\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "# ChatGPT local analysis in Jupyter Notebook ü§ñ\n",
       "\n",
       "In this notebook I deliver a foundation on how to analyse the personal ChatGPT conversation history. \n",
       "\n",
       "After a few months of using ChatGPT, I have collected a large amount of data and want to reflect on old conversations and spark ideas on how to use it in the future.\n",
       "\n",
       "Using the Superpower ChatGPT extension for Chrome you can automatically sync your conversations for offline usage. Since all your conversations are now stored locally, you can analyse the local database to get insights into your conversations.\n",
       "\n",
       "## Demo\n",
       "Feel free to check out the notebook [here](./chatgpt_analysis.ipynb): \n",
       "## Requirements\n",
       "\n",
       "- Superpower ChatGPT extension: https://github.com/saeedezzati/superpower-chatgpt\n",
       "- Python 3.8\n",
       "- Streamlit\n",
       "\n",
       "\n",
       "## Current state\n",
       "\n",
       "- [x] Get data from local database to a pandas dataframe\n",
       "    - [x] df_conversations\n",
       "    - [x] df_messages\n",
       "\n",
       "- [x] Sync ChatGPT conversations to Notion\n",
       "    - [x] Add a Notion token to your .env file\n",
       "    - [x] Just add a 'üìù' emoji to the conversation and it will be synced to Notion\n",
       "    - [x] Avoid 2000 character limit by splitting the message into multiple messages\n",
       "    - [ ] Add a link to the original conversation in the Notion page\n",
       "    - [ ] Let it detect changes in already synced conversations\n",
       "\n",
       "- [x] Streamlint Dashboard\n",
       "    - [x] Basic setup\n",
       "    - [x] Table with conversations\n",
       "    - [x] Wordcloud of all messages\n",
       "    - [x] Exclusion list in custom_stop_words.txt\n",
       "    \n",
       "    - [ ] Conversation overview\n",
       "    - [ ] Graph network of conversations / words / topics\n",
       "\n",
       "## Future features\n",
       "\n",
       "- Graph network of conversations / words / topics (Can someone help me with this?) \n",
       "    - Not sure how to do this, but I think it would be cool to see how conversations are connected and how topics are connected to each other\n",
       "    - https://towardsdatascience.com/how-to-deploy-interactive-pyvis-network-graphs-on-streamlit-6c401d4c99db\n",
       "\n",
       "## Pull requests are welcome!\n",
       "Feel free to contribute to this project and contribute ideas on how to analyse the data."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import readme.md file (project overview) and display the markdown file here\n",
    "\n",
    "from IPython.core.display import display, Markdown\n",
    "\n",
    "with open('README.md', 'r') as f:\n",
    "    content = f.read()\n",
    "\n",
    "display(Markdown(content))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install packages below\n",
    "\n",
    "# !pip install pandas\n",
    "\n",
    "\n",
    "import plyvel\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data from browser database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 123704\n",
      "-rw-------@ 1 phil  staff   7720330 May 30 17:41 003033.ldb\n",
      "-rw-------@ 1 phil  staff    137144 Jun 10 20:07 007108.ldb\n",
      "-rw-------@ 1 phil  staff   8014764 Jun 11 02:14 007183.ldb\n",
      "-rw-------@ 1 phil  staff    136460 Jun 11 02:14 007184.ldb\n",
      "-rw-------@ 1 phil  staff   8057368 Jun 11 02:14 007186.ldb\n",
      "-rw-------@ 1 phil  staff   7824187 Jun 11 02:15 007188.ldb\n",
      "-rw-------@ 1 phil  staff  22414693 Jun 11 03:05 007189.log\n",
      "-rw-------@ 1 phil  staff   7817576 Jun 11 02:44 007190.ldb\n",
      "-rw-------@ 1 phil  staff        16 May 28 21:05 CURRENT\n",
      "-rw-------@ 1 phil  staff         0 May 28 21:05 LOCK\n",
      "-rw-------@ 1 phil  staff     11766 Jun 11 02:44 LOG\n",
      "-rw-------@ 1 phil  staff     76841 Jun 10 17:55 LOG.old\n",
      "-rw-------@ 1 phil  staff    327119 Jun 11 02:44 MANIFEST-000001\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Path to the leveldb directory (change this to your path - in my case I am using Brave Browser)\n",
    "leveldb_path = '~/Library/Application Support/BraveSoftware/Brave-Browser/Default/Local Extension Settings/amhmeenmapldpjdedekalnfifgnpfnkc'\n",
    "leveldb_path = os.path.expanduser(leveldb_path)  # Expand the '~' symbol to the user's home directory\n",
    "\n",
    "# Enclose the path in quotes to handle spaces\n",
    "leveldb_path = f'\"{leveldb_path}\"'\n",
    "\n",
    "# Get the list of files in the leveldb directory\n",
    "\n",
    "!ls -l $leveldb_path\n",
    "\n",
    "# Copy all files to the current directory /db\n",
    "\n",
    "!cp -r $leveldb_path ./db\n",
    "\n",
    "# set leveldb path to the copied directory\n",
    "\n",
    "leveldb_path = './db/amhmeenmapldpjdedekalnfifgnpfnkc'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initiate Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'                               Compactions\\nLevel  Files Size(MB) Time(sec) Read(MB) Write(MB)\\n--------------------------------------------------\\n  0        4       30         0        0         7\\n  1        2        8         0        0         0\\n  2        2        7         0        0         0\\n'\n"
     ]
    }
   ],
   "source": [
    "db = plyvel.DB(leveldb_path)\n",
    "\n",
    "# get database info\n",
    "\n",
    "print(db.get_property(b'leveldb.stats'))\n",
    "\n",
    "\n",
    "# filter all key entries that contain chat.openai.com \n",
    "\n",
    "for key, value in db:\n",
    "    if b'chat.openai.com' in key:\n",
    "        print(f'Key: {key}, Value: {value}')\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.DataFrame(columns=['key', 'value'])\n",
    "\n",
    "for key, value in db:\n",
    "    df = pd.concat([df, pd.DataFrame({'key': [key], 'value': [value]})], ignore_index=True)\n",
    "    # df = df.append({'key': key, 'value': value}, ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the value of key \"conversations\"  as json in a separate dataframe\n",
    "\n",
    "df_conversations = pd.DataFrame(json.loads(df[df['key'] == b'conversations']['value'].values[0]))\n",
    "# with column / lines transposed\n",
    "df_conversations = df_conversations.T\n",
    "# also export as text file\n",
    "df_conversations.to_csv('df_conversations.txt', sep='\\t', index=False)\n",
    "\n",
    "# # add a column for the key value length\n",
    "df['key_length'] = df['value'].str.len()\n",
    "\n",
    "df.head()\n",
    "\n",
    "# close the database\n",
    "db.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     id                       conversation_id  \\\n",
      "0  430741cb-bd4b-44bd-b38a-fffeb45a3b49  000250bf-4b4b-47de-9bb9-2425ebccf8c0   \n",
      "1  72ed73e5-3856-4b72-9700-23dc8acd0015  000250bf-4b4b-47de-9bb9-2425ebccf8c0   \n",
      "2  af30cc11-ce74-475a-8a21-aa9d375ea1e7  000250bf-4b4b-47de-9bb9-2425ebccf8c0   \n",
      "3  eea89b97-3e61-424c-9f92-c57d7706973f  000250bf-4b4b-47de-9bb9-2425ebccf8c0   \n",
      "4  f248e179-f8fd-4184-8163-20a44ab381fa  000250bf-4b4b-47de-9bb9-2425ebccf8c0   \n",
      "\n",
      "      title                                            message       role  \\\n",
      "0  New chat  Any ideas for a cycling-related data-driven gr...       user   \n",
      "1  New chat                                                        system   \n",
      "2  New chat  Any ideas for a data-driven graduation project...       user   \n",
      "3  New chat  Here are a few ideas for a cycling-related dat...  assistant   \n",
      "4  New chat  Here are a few ideas for a data-driven graduat...  assistant   \n",
      "\n",
      "         create_time  \n",
      "0  1671022256.202531  \n",
      "1  1671021508.989812  \n",
      "2  1671021508.990174  \n",
      "3  1671022299.506987  \n",
      "4  1671021555.109059  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# drop all rows of df_conversations with empty mapping\n",
    "df_conversations = df_conversations[df_conversations['mapping'].str.len() > 0]\n",
    "\n",
    "# Create a new DataFrame for messages\n",
    "df_messages = pd.DataFrame(columns=['id','conversation_id', 'title', 'message', 'role', 'create_time'])\n",
    "\n",
    "# Iterate over the conversations and messages\n",
    "for idx, conversation in df_conversations.iterrows():\n",
    "    mapping = conversation['mapping']\n",
    "    title = conversation['title']\n",
    "    conversation_id = conversation['id']\n",
    "    \n",
    "    # Iterate over the messages in the mapping\n",
    "    for message_id, message_data in mapping.items():\n",
    "        if 'message' in message_data and message_data['message'] is not None:\n",
    "            message = message_data['message']\n",
    "            \n",
    "            # Get the message text\n",
    "            message_text = ''\n",
    "            if 'content' in message and 'parts' in message['content']:\n",
    "                message_parts = message['content']['parts']\n",
    "                if len(message_parts) > 0:\n",
    "                    message_text = message_parts[0]\n",
    "            \n",
    "            # Get the author role\n",
    "            role = ''\n",
    "            if 'author' in message and 'role' in message['author']:\n",
    "                role = message['author']['role']\n",
    "            \n",
    "            # Get the create time\n",
    "            create_time = ''\n",
    "            if 'create_time' in message:\n",
    "                create_time = message['create_time']\n",
    "            \n",
    "            # Append the message data to the DataFrame\n",
    "            # df_messages = df_messages.append({\n",
    "            #     'id': message_id,\n",
    "            #     'title': title,\n",
    "            #     'message': message_text,\n",
    "            #     'role': role,\n",
    "            #     'create_time': create_time\n",
    "            # }, ignore_index=True)\n",
    "\n",
    "            df_messages = pd.concat([df_messages, pd.DataFrame({\n",
    "                'id': [message_id],\n",
    "                'conversation_id': [conversation_id],\n",
    "                'title': [title],\n",
    "                'message': [message_text],\n",
    "                'role': [role],\n",
    "                'create_time': [create_time]\n",
    "                })], ignore_index=True)\n",
    "            \n",
    "\n",
    "\n",
    "# Close the LevelDB database\n",
    "db.close()\n",
    "\n",
    "# Display the first few rows of the messages DataFrame\n",
    "print(df_messages.head())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sync to Notion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>conversation_id</th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>üìù Card Game: Food Gods</td>\n",
       "      <td>1957a0eb-5b02-458c-bcda-848cea8c725b</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>üìù ImmoTechGo - Revolutive Immobilienl√∂sungen</td>\n",
       "      <td>0f0a1626-b3bf-4144-9d40-1a75e9a800d7</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>üìù Lindner: Freiheit auf Autobahnen!</td>\n",
       "      <td>f5efaecf-66f0-40db-a49a-4ac7042e6d05</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          title  \\\n",
       "0                        üìù Card Game: Food Gods   \n",
       "1  üìù ImmoTechGo - Revolutive Immobilienl√∂sungen   \n",
       "2           üìù Lindner: Freiheit auf Autobahnen!   \n",
       "\n",
       "                        conversation_id  counts  \n",
       "0  1957a0eb-5b02-458c-bcda-848cea8c725b       6  \n",
       "1  0f0a1626-b3bf-4144-9d40-1a75e9a800d7       2  \n",
       "2  f5efaecf-66f0-40db-a49a-4ac7042e6d05       3  "
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter df messages to where title contains 'üìù' as new dataframe df_notion\n",
    "df_notion = df_messages[df_messages['title'].str.contains('üìù')]\n",
    "\n",
    "# drop all lines where role is 'system' or empty\n",
    "df_notion = df_notion[df_notion['role'] != 'system']\n",
    "\n",
    "# format create_time as datetime\n",
    "df_notion['create_time'] = pd.to_datetime(df_notion['create_time'], unit='s')\n",
    "# drop all lines where create_time is no datetime\n",
    "df_notion = df_notion[df_notion['create_time'].notnull()]\n",
    "\n",
    "# sort by create_time (oldest first)\n",
    "df_notion = df_notion.sort_values(by=['create_time'])\n",
    "\n",
    "\n",
    "\n",
    "# list all unique titles along with the conversation_id and the number of messages\n",
    "\n",
    "df_notion.groupby(['title', 'conversation_id']).size().reset_index(name='counts')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Install notion-api\n",
    "# !pip install notion-api\n",
    "\n",
    "import dotenv\n",
    "import notion\n",
    "\n",
    "# client will check env variables for 'NOTION_TOKEN' \n",
    "dotenv.load_dotenv() # take environment variables from .env.\n",
    "\n",
    "\n",
    "homepage = notion.Page('fc357867d5164d29bc2e1c8231c98284')\n",
    "parent_db = notion.Database(homepage.parent_id)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Template',\n",
       " 'c60f7a077a26483090887c9e750d9154',\n",
       " 'fc357867d5164d29bc2e1c8231c98284',\n",
       " 'https://www.notion.so/Template-fc357867d5164d29bc2e1c8231c98284')"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "homepage.title, homepage.parent_id, homepage.id, homepage.url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Lindner: Freiheit auf Autobahnen! f5efaecf-66f0-40db-a49a-4ac7042e6d05 bde74a0b1df34e4d8a8ca260e6f0706d https://www.notion.so/Lindner-Freiheit-auf-Autobahnen-bde74a0b1df34e4d8a8ca260e6f0706d\n",
      "‚úÖ found in df_notion\n",
      "______________________\n",
      "üìù ImmoTechGo - Revolutive Immobilienl√∂sungen 0f0a1626-b3bf-4144-9d40-1a75e9a800d7 94b2ddc62b5b445c871fceb97b9c3caa https://www.notion.so/ImmoTechGo-Revolutive-Immobilienl-sungen-94b2ddc62b5b445c871fceb97b9c3caa\n",
      "‚úÖ found in df_notion\n",
      "______________________\n",
      "1957a0eb-5b02-458c-bcda-848cea8c725b\n",
      "‚ú® 1957a0eb-5b02-458c-bcda-848cea8c725b not found in database - will be added to pages_to_sync\n",
      "0f0a1626-b3bf-4144-9d40-1a75e9a800d7\n",
      "ü•∑ 0f0a1626-b3bf-4144-9d40-1a75e9a800d7 found in database - will be ignored\n",
      "f5efaecf-66f0-40db-a49a-4ac7042e6d05\n",
      "ü•∑ f5efaecf-66f0-40db-a49a-4ac7042e6d05 found in database - will be ignored\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "from notion import query\n",
    "\n",
    "query_result = parent_db.query_pages()\n",
    "pages_to_sync = []\n",
    "\n",
    "# find all pages and their conversation_id and check if they are in df_notion\n",
    "for page in query_result:\n",
    "\n",
    "    try:\n",
    "        conversation_id = page['conversation_id']['rich_text'][0]['plain_text']\n",
    "    except:\n",
    "        # no conversation_id\n",
    "        conversation_id = None\n",
    "        # add to list to be synced\n",
    "        # create new variable if not already existing\n",
    "        # pages_to_sync.append(page)\n",
    "\n",
    "    print(page.title, conversation_id, page.id, page.url, )\n",
    "    if conversation_id in df_notion['conversation_id'].values:\n",
    "        print('‚úÖ found in df_notion')\n",
    "    else:\n",
    "        print('ü•∑ not found in df_notion - will be ignored')\n",
    "    print('______________________')\n",
    "\n",
    "# find the unique remaining pages in df_notion that are not yet in the database and add them to the list to be synced\n",
    "\n",
    "\n",
    "# extract all conversation_ids from query_result\n",
    "\n",
    "query_result_conversation_ids = [page['conversation_id']['rich_text'][0]['plain_text'] for page in query_result if page['conversation_id'] is not None]\n",
    "\n",
    "for conversation_id in df_notion['conversation_id'].unique():\n",
    "    print(conversation_id)\n",
    "    if conversation_id not in query_result_conversation_ids:\n",
    "        print (f'‚ú® {conversation_id} not yet found in database - will be added to pages_to_sync')\n",
    "        pages_to_sync.append(conversation_id)\n",
    "    else:\n",
    "        print (f'ü•∑ {conversation_id} found in database - will be ignored')\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1957a0eb-5b02-458c-bcda-848cea8c725b']"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages_to_sync   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create new pages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Card Game: Food Gods\n",
      "['Invent a competitive card came with mystical food gods and their features'\n",
      " 'Sure! Here is a card game featuring mystical food gods and their unique abilities:\\n\\nThe game is called \"Feast of the Gods\" and is played with a deck of 60 cards. Each card represents a different food god, with their own special power and a corresponding food type.\\n\\nThe goal of the game is to be the last player standing by eliminating all of the other players\\' food gods. Players can attack and defend using the powers of their food gods, and can also play action cards to change the course of the game.\\n\\nHere are a few examples of food gods and their abilities:\\n\\n- \"Papa Corn\" (Corn): Papa Corn has the power to convert any food god into a weaker version of itself. For example, if Papa Corn attacks \"Mama Tomato\" (Tomato), Mama Tomato becomes a \"Baby Tomato\" with half the power and health of its original form.\\n\\n- \"King Coconut\" (Coconut): King Coconut has the power to deflect any attack back at the attacker. If another food god attacks King Coconut, the attack is reflected back at the attacking food god, dealing the same amount of damage.\\n\\n- \"Queen Quinoa\" (Quinoa): Queen Quinoa has the ability to heal any food god, including herself. Whenever Queen Quinoa is played, she can choose any food god on the field and restore some of its health.\\n\\n- \"Elderberry\" (Elderberry): Elderberry has the power to remove any action card from play. If an opponent plays an action card that Elderberry doesn\\'t like, Elderberry can simply remove it from the game.\\n\\nThese are just a few examples of the many food gods and their abilities that could be included in the Feast of the Gods card game. The strategy and tactics of the game would come from choosing the right food gods to play at the right time and using their abilities effectively.'\n",
      " 'can you invent some more examples?'\n",
      " 'Certainly! Here are a few more examples of food gods and their abilities for the Feast of the Gods card game:\\n\\n- \"Mama Mushroom\" (Mushroom): Mama Mushroom has the power to absorb damage from other food gods. Whenever Mama Mushroom is attacked, she can choose to absorb the damage, taking no damage herself and instead adding it to her own power.\\n\\n- \"Big Cheese\" (Cheese): Big Cheese has the ability to steal any food god\\'s ability for a single turn. If Big Cheese attacks a food god, he can choose to steal that food god\\'s ability and use it himself for the rest of the turn.\\n\\n- \"Sugar Queen\" (Sugar): Sugar Queen has the power to double the power of any food god for a single turn. If Sugar Queen is played alongside another food god, that food god\\'s power is doubled for the rest of the turn.\\n\\n- \"Salty Sam\" (Salt): Salty Sam has the ability to cancel out any food god\\'s special ability for a single turn. If Salty Sam is played, he can choose a food god on the field and disable its special ability for the rest of the turn.\\n\\n- \"Ginger King\" (Ginger): Ginger King has the power to counter any attack. If another food god attacks Ginger King, he can choose to redirect the attack back at the attacker, dealing double the damage.\\n\\n- \"Mint Mistress\" (Mint): Mint Mistress has the ability to cleanse any food god, removing any negative effects or debuffs. If Mint Mistress is played, she can choose a food god and remove any negative effects or debuffs it may be suffering from.\\n\\nI hope these additional examples give you some ideas for more food gods and their abilities in the Feast of the Gods card game!'\n",
      " 'can you invent one related to pasta?'\n",
      " 'Sure! Here is a food god related to pasta for the Feast of the Gods card game:\\n\\n- \"Pasta Princess\" (Pasta): Pasta Princess has the power to create copies of any food god in play. When Pasta Princess is played, she can choose any food god on the field and create a copy of it, which is added to the player\\'s hand. The copy has the same power and abilities as the original food god.\\n\\nPasta Princess\\'s ability allows a player to quickly build up their army of food gods and overwhelm their opponents. However, it can also be risky, as it allows the opponent to potentially get more copies of their own powerful food gods as well. Players will need to weigh the benefits and drawbacks of using Pasta Princess\\'s ability carefully.']\n",
      "80acd0d1910845a194f27c8664640cea\n",
      "‚ú® üìù Card Game: Food Gods added to database\n"
     ]
    }
   ],
   "source": [
    "# for each page retrieve the page data from the df_notion dataframe\n",
    "\n",
    "# for page in pages_to_sync: \n",
    "#     print(page)\n",
    "#     # print(page.title)\n",
    "#     print(df_notion[df_notion['conversation_id'] == page]['message'].values)\n",
    "#     print('______________________')    \n",
    "\n",
    "# for each pages_to_sync, create a new page in the notion database with the title of the conversation_id and the content of the messages \n",
    "from notion import properties as prop\n",
    "\n",
    "\n",
    "for page in pages_to_sync:\n",
    "    title = df_notion[df_notion['conversation_id'] == page]['title'].values[0]\n",
    "    conversation_id = df_notion[df_notion['conversation_id'] == page]['conversation_id'].values[0]\n",
    "    print (title)\n",
    "    messages = df_notion[df_notion['conversation_id'] == page]['message'].values\n",
    "    print (messages)\n",
    "    notion_page = notion.Page.create(parent_db, page_title=title)\n",
    "    # page.children.add_new(notion.blocks.TextBlock, title='test')\n",
    "    # get the page id of the just created page using query\n",
    "    print (notion_page.id)\n",
    "\n",
    "    notion_page.set_text('conversation_id', conversation_id)\n",
    "    # add the messages to the page\n",
    "    for message in messages:\n",
    "         # add emoji based on role\n",
    "        if df_notion[df_notion['message'] == message]['role'].values[0] == 'user':\n",
    "            notion.Block.heading2(notion_page, [prop.RichText('üíÅ‚Äç‚ôÇÔ∏è')])\n",
    "        else:\n",
    "            notion.Block.heading2(notion_page, [prop.RichText('ü§ñ')])\n",
    "\n",
    "        # if message is > 2000 characters, split it into multiple blocks to avoid error\n",
    "        if len(message) > 2000:\n",
    "            # split the message into multiple blocks\n",
    "            blocks = [message[i:i+2000] for i in range(0, len(message), 2000)]\n",
    "            # add each block to the page\n",
    "            for block in blocks:\n",
    "                notion.Block.quote(notion_page, [prop.RichText(block)])\n",
    "\n",
    "        else:\n",
    "            notion.Block.quote(notion_page, [prop.RichText(message)])\n",
    "    print(f'‚ú® {title} added to database')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning for word analysis\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # install nltk\n",
    "# # !pip install nltk\n",
    "# import nltk\n",
    "# from nltk.corpus import stopwords\n",
    "# from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "# # add more stop words in english and german\n",
    "# stop_words = stopwords.words('english')\n",
    "# # also exclude code words like < >, the, to, and =, a, in , of, for [ ] ( ) {} etc.\n",
    "# code_words = ['<', '>', 'the', 'to', 'and', '=', 'a', 'in', 'of', 'for', '[', ']', '(', ')', '{', '}', 'const', 'import', 'script', 'button', 'await', 'null', 'code', 'div', 'und', 'px', 'data', 'file', 'die', 'return', 'image', 'user', 'der', 'use', 'error', 'value', 'new', 'color', 'zu', 'create', 'using', 'component', 'add', 'false', 'object', 'template', 'name', 'da', 'also', 'app', 'example', 'span', 'f√ºr', 'width', 'mit', 'type', 'content', 'label', 'method', 'display', 'feedback', 'bike', 'rating', 'style', 'location', 'e', 'backgroundcolor', 'try', 'height', 'center', 'button', 'title', 'div', 'px', 'color', 'null', 'file', 'template', 'da', 'false', 'value', 'script', 'span', 'error', 'backgroundcolor', 'e', 'button', 'sie', 'ist', 'true', 'cycling', 'make', 'eine', 'class', 'default', 'auf', 'could', 'von', 'heres', 'like', 'name', 'function', 'used', 'i', 'need', 'async', 'based', 'label', 'data', 'advice', 'style', 'center', 'submission', 'infrastructure', 'set', 'model', 'property', 'width', 'type', 'id', 'ein', 'report', 'div', 'px', 'color', 'null', 'file', 'template', 'da', 'false', 'value', 'script', 'span', 'error', 'backgroundcolor', 'e', 'button', 'heres', 'name', 'i', 'label', 'data', 'style', 'center', 'width', 'type', 'index', 'text', 'vue', 'map', 'get', 'den', 'title', 'image', 'user', 'issue', 'array', 'f', 'column', 'comment', 'element', 'true', 'list', 'display', 'p', 'sure', 'im', 'result', 'height']\n",
    "\n",
    "# lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# df_messages['cleaned_text'] = df_messages['message'].str.lower()\n",
    "# # strip out words from stop_words and code_words from cleaned_text column\n",
    "# df_messages['cleaned_text'] = df_messages['cleaned_text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words + code_words)]))\n",
    "# df_messages['cleaned_text'] = df_messages['cleaned_text'].apply(lambda x: ' '.join([lemmatizer.lemmatize(word) for word in x.split()]))\n",
    "# df_messages['cleaned_text'] = df_messages['cleaned_text'].str.replace('[^\\w\\s]','')\n",
    "# df_messages['cleaned_text'] = df_messages['cleaned_text'].str.replace('[\\d+]','')\n",
    "\n",
    "# df_messages.head()\n",
    "\n",
    "# # based on df_messages['cleaned_text'] column create word frequency count \n",
    "\n",
    "# from collections import Counter\n",
    "\n",
    "# # Create a list of all words in the messages\n",
    "# all_words = []\n",
    "\n",
    "# for idx, row in df_messages.iterrows():\n",
    "#     all_words.extend(row['cleaned_text'].split())\n",
    "\n",
    "# # Create a word frequency counter\n",
    "\n",
    "# word_freq = Counter(all_words)\n",
    "\n",
    "# # Display the 10 most common words\n",
    "\n",
    "# print(word_freq.most_common(10))\n",
    "\n",
    "# # Create a list of all words in the messages\n",
    "\n",
    "# all_words = []\n",
    "\n",
    "# # save the messages dataframe as txt\n",
    "\n",
    "# df_messages.to_csv('df_messages.txt', sep='\\t', index=False)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install pyvis and jinja2\n",
    "# !pip install pyvis\n",
    "# !pip install jinja2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dashboard\n",
    "\n",
    "Initialize the streamlint dashboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install streamlit\n",
    "# !pip install streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install worldcloud\n",
    "# !pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install networkx\n",
    "# %pip install networkx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run streamlit app\n",
    "\n",
    "code = \"\"\"\n",
    "\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "# read custom_stop_words from txt file\n",
    "\n",
    "custom_stop_words = []\n",
    "with open('custom_stop_words.txt', 'r') as f:\n",
    "    custom_stop_words = f.read().splitlines()\n",
    "\n",
    "\n",
    "# Read the chat logs data into a DataFrame\n",
    "df_messages = pd.read_csv('df_messages.txt', sep='\t')\n",
    "df_messages['cleaned_text'] = df_messages['cleaned_text'].astype(str)  # Convert 'cleaned_text' column to string type\n",
    "# filter out code words in cleaned_text column \n",
    "df_messages['cleaned_text'] = df_messages['cleaned_text'].apply(lambda x: ' '.join([word for word in x.split() if word not in custom_stop_words]))\n",
    "\n",
    "# Process the chat messages to extract words\n",
    "all_words = ' '.join(df_messages['cleaned_text']).lower().split()\n",
    "all_words = [word for word in all_words if word not in custom_stop_words]\n",
    "word_counts = Counter(all_words)\n",
    "most_common_words = word_counts.most_common(200)  # Change the number as per your requirement\n",
    "\n",
    "# Create a DataFrame for the most common words\n",
    "df_common_words = pd.DataFrame(most_common_words, columns=['Word', 'Count'])\n",
    "\n",
    "# Display the most common words in a bar chart\n",
    "fig, ax = plt.subplots()\n",
    "df_common_words.plot.bar(x='Word', y='Count', ax=ax)\n",
    "plt.xlabel('Word')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Most Common Words in Chat Logs')\n",
    "plt.xticks(rotation=45)\n",
    "st.pyplot(fig)\n",
    "\n",
    "# Display the raw data of the most common words\n",
    "st.write(df_common_words)\n",
    "\n",
    "# Display the raw data of the chat logs\n",
    "st.write(df_messages)\n",
    "\n",
    "# display a word cloud\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Create a word cloud\n",
    "\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white', max_words=200).generate(' '.join(df_messages['cleaned_text']))\n",
    "\n",
    "# Display the generated image:\n",
    "# the matplotlib way:\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(wordcloud, interpolation='bilinear')\n",
    "ax.set_axis_off()\n",
    "st.pyplot(fig)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the code to the dashboard.py file\n",
    "with open('dashboard.py', 'w') as f:\n",
    "    f.write(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: streamlit\n"
     ]
    }
   ],
   "source": [
    "# run streamlit app dashboard.py\n",
    "!streamlit run dashboard.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# close the database\n",
    "db.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "general_notebook_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
