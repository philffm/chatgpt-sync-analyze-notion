{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wc/1fgpcwpx4rzfbbc7shb_wy8h0000gn/T/ipykernel_27253/2235448322.py:3: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, Markdown\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "# ChatGPT local analysis in Jupyter Notebook ü§ñ & Sync to Notion üìù\n",
       "\n",
       "In this notebook I deliver a foundation on how to analyse the personal ChatGPT conversation history. \n",
       "\n",
       "After a few months of using ChatGPT, I have collected a large amount of data and want to reflect on old conversations and spark ideas on how to use it in the future.\n",
       "\n",
       "Using the Superpower ChatGPT extension for Chrome you can automatically sync your conversations for offline usage. Since all your conversations are now stored locally, you can analyse the local database to get insights into your conversations.\n",
       "\n",
       "## Demo\n",
       "Feel free to check out the notebook [here](./chatgpt_analysis.ipynb): \n",
       "## Requirements\n",
       "\n",
       "- Superpower ChatGPT extension: https://github.com/saeedezzati/superpower-chatgpt\n",
       "- Python 3.8\n",
       "- Streamlit\n",
       "\n",
       "\n",
       "## Current state\n",
       "\n",
       "- [x] Get data from local database to a pandas dataframe\n",
       "    - [x] df_conversations\n",
       "    - [x] df_messages\n",
       "\n",
       "- [x] Sync ChatGPT conversations to Notion\n",
       "    - [x] Add a Notion token to your .env file\n",
       "    - [x] Just add a 'üìù' emoji to the conversation and it will be synced to Notion\n",
       "    - [x] Avoid 2000 character limit by splitting the message into multiple messages\n",
       "    - [ ] Add a link to the original conversation in the Notion page\n",
       "    - [ ] Let it detect changes in already synced conversations\n",
       "\n",
       "- [x] Streamlint Dashboard\n",
       "    - [x] Basic setup\n",
       "    - [x] Table with conversations\n",
       "    - [x] Wordcloud of all messages\n",
       "    - [x] Exclusion list in custom_stop_words.txt\n",
       "    \n",
       "    - [ ] Conversation overview\n",
       "    - [ ] Graph network of conversations / words / topics\n",
       "\n",
       "## Future features\n",
       "\n",
       "- Graph network of conversations / words / topics (Can someone help me with this?) \n",
       "    - Not sure how to do this, but I think it would be cool to see how conversations are connected and how topics are connected to each other\n",
       "    - https://towardsdatascience.com/how-to-deploy-interactive-pyvis-network-graphs-on-streamlit-6c401d4c99db\n",
       "\n",
       "## Pull requests are welcome!\n",
       "Feel free to contribute to this project and contribute ideas on how to analyse the data."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import readme.md file (project overview) and display the markdown file here\n",
    "\n",
    "from IPython.core.display import display, Markdown\n",
    "\n",
    "with open('README.md', 'r') as f:\n",
    "    content = f.read()\n",
    "\n",
    "display(Markdown(content))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install packages below\n",
    "\n",
    "# !pip install pandas\n",
    "\n",
    "\n",
    "import plyvel\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data from browser database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 121960\n",
      "-rw-------@ 1 phil  staff   7720330 May 30 17:41 003033.ldb\n",
      "-rw-------@ 1 phil  staff    137144 Jun 10 20:07 007108.ldb\n",
      "-rw-------@ 1 phil  staff   8010562 Jun 11 03:15 007203.ldb\n",
      "-rw-------@ 1 phil  staff    138701 Jun 11 03:15 007204.ldb\n",
      "-rw-------@ 1 phil  staff   7819005 Jun 11 03:15 007206.ldb\n",
      "-rw-------@ 1 phil  staff   7820882 Jun 11 03:16 007208.ldb\n",
      "-rw-------@ 1 phil  staff  22419727 Jun 11 03:16 007209.log\n",
      "-rw-------@ 1 phil  staff   7815952 Jun 11 03:16 007210.ldb\n",
      "-rw-------@ 1 phil  staff        16 May 28 21:05 CURRENT\n",
      "-rw-------@ 1 phil  staff         0 May 28 21:05 LOCK\n",
      "-rw-------@ 1 phil  staff     14436 Jun 11 03:16 LOG\n",
      "-rw-------@ 1 phil  staff     76841 Jun 10 17:55 LOG.old\n",
      "-rw-------@ 1 phil  staff    328093 Jun 11 03:16 MANIFEST-000001\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Path to the leveldb directory (change this to your path - in my case I am using Brave Browser)\n",
    "leveldb_path = '~/Library/Application Support/BraveSoftware/Brave-Browser/Default/Local Extension Settings/amhmeenmapldpjdedekalnfifgnpfnkc'\n",
    "leveldb_path = os.path.expanduser(leveldb_path)  # Expand the '~' symbol to the user's home directory\n",
    "\n",
    "# Enclose the path in quotes to handle spaces\n",
    "leveldb_path = f'\"{leveldb_path}\"'\n",
    "\n",
    "# Get the list of files in the leveldb directory\n",
    "\n",
    "!ls -l $leveldb_path\n",
    "\n",
    "# Copy all files to the current directory /db\n",
    "\n",
    "!cp -r $leveldb_path ./db\n",
    "\n",
    "# set leveldb path to the copied directory\n",
    "\n",
    "leveldb_path = './db/amhmeenmapldpjdedekalnfifgnpfnkc'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initiate Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'                               Compactions\\nLevel  Files Size(MB) Time(sec) Read(MB) Write(MB)\\n--------------------------------------------------\\n  0        4       30         0        0         7\\n  1        2        8         0        0         0\\n  2        2        7         0        0         0\\n'\n"
     ]
    }
   ],
   "source": [
    "db = plyvel.DB(leveldb_path)\n",
    "\n",
    "# get database info\n",
    "\n",
    "print(db.get_property(b'leveldb.stats'))\n",
    "\n",
    "\n",
    "# filter all key entries that contain chat.openai.com \n",
    "\n",
    "for key, value in db:\n",
    "    if b'chat.openai.com' in key:\n",
    "        print(f'Key: {key}, Value: {value}')\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.DataFrame(columns=['key', 'value'])\n",
    "\n",
    "for key, value in db:\n",
    "    df = pd.concat([df, pd.DataFrame({'key': [key], 'value': [value]})], ignore_index=True)\n",
    "    # df = df.append({'key': key, 'value': value}, ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the value of key \"conversations\"  as json in a separate dataframe\n",
    "\n",
    "df_conversations = pd.DataFrame(json.loads(df[df['key'] == b'conversations']['value'].values[0]))\n",
    "# with column / lines transposed\n",
    "df_conversations = df_conversations.T\n",
    "# also export as text file\n",
    "df_conversations.to_csv('df_conversations.txt', sep='\\t', index=False)\n",
    "\n",
    "# # add a column for the key value length\n",
    "df['key_length'] = df['value'].str.len()\n",
    "\n",
    "df.head()\n",
    "\n",
    "# close the database\n",
    "db.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     id                       conversation_id  \\\n",
      "0  430741cb-bd4b-44bd-b38a-fffeb45a3b49  000250bf-4b4b-47de-9bb9-2425ebccf8c0   \n",
      "1  72ed73e5-3856-4b72-9700-23dc8acd0015  000250bf-4b4b-47de-9bb9-2425ebccf8c0   \n",
      "2  af30cc11-ce74-475a-8a21-aa9d375ea1e7  000250bf-4b4b-47de-9bb9-2425ebccf8c0   \n",
      "3  eea89b97-3e61-424c-9f92-c57d7706973f  000250bf-4b4b-47de-9bb9-2425ebccf8c0   \n",
      "4  f248e179-f8fd-4184-8163-20a44ab381fa  000250bf-4b4b-47de-9bb9-2425ebccf8c0   \n",
      "\n",
      "      title                                            message       role  \\\n",
      "0  New chat  Any ideas for a cycling-related data-driven gr...       user   \n",
      "1  New chat                                                        system   \n",
      "2  New chat  Any ideas for a data-driven graduation project...       user   \n",
      "3  New chat  Here are a few ideas for a cycling-related dat...  assistant   \n",
      "4  New chat  Here are a few ideas for a data-driven graduat...  assistant   \n",
      "\n",
      "         create_time  \n",
      "0  1671022256.202531  \n",
      "1  1671021508.989812  \n",
      "2  1671021508.990174  \n",
      "3  1671022299.506987  \n",
      "4  1671021555.109059  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# drop all rows of df_conversations with empty mapping\n",
    "df_conversations = df_conversations[df_conversations['mapping'].str.len() > 0]\n",
    "\n",
    "# Create a new DataFrame for messages\n",
    "df_messages = pd.DataFrame(columns=['id','conversation_id', 'title', 'message', 'role', 'create_time'])\n",
    "\n",
    "# Iterate over the conversations and messages\n",
    "for idx, conversation in df_conversations.iterrows():\n",
    "    mapping = conversation['mapping']\n",
    "    title = conversation['title']\n",
    "    conversation_id = conversation['id']\n",
    "    \n",
    "    # Iterate over the messages in the mapping\n",
    "    for message_id, message_data in mapping.items():\n",
    "        if 'message' in message_data and message_data['message'] is not None:\n",
    "            message = message_data['message']\n",
    "            \n",
    "            # Get the message text\n",
    "            message_text = ''\n",
    "            if 'content' in message and 'parts' in message['content']:\n",
    "                message_parts = message['content']['parts']\n",
    "                if len(message_parts) > 0:\n",
    "                    message_text = message_parts[0]\n",
    "            \n",
    "            # Get the author role\n",
    "            role = ''\n",
    "            if 'author' in message and 'role' in message['author']:\n",
    "                role = message['author']['role']\n",
    "            \n",
    "            # Get the create time\n",
    "            create_time = ''\n",
    "            if 'create_time' in message:\n",
    "                create_time = message['create_time']\n",
    "            \n",
    "            # Append the message data to the DataFrame\n",
    "            # df_messages = df_messages.append({\n",
    "            #     'id': message_id,\n",
    "            #     'title': title,\n",
    "            #     'message': message_text,\n",
    "            #     'role': role,\n",
    "            #     'create_time': create_time\n",
    "            # }, ignore_index=True)\n",
    "\n",
    "            df_messages = pd.concat([df_messages, pd.DataFrame({\n",
    "                'id': [message_id],\n",
    "                'conversation_id': [conversation_id],\n",
    "                'title': [title],\n",
    "                'message': [message_text],\n",
    "                'role': [role],\n",
    "                'create_time': [create_time]\n",
    "                })], ignore_index=True)\n",
    "            \n",
    "\n",
    "\n",
    "# Close the LevelDB database\n",
    "db.close()\n",
    "\n",
    "# Display the first few rows of the messages DataFrame\n",
    "print(df_messages.head())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sync to Notion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>conversation_id</th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>üìù Card Game: Food Gods</td>\n",
       "      <td>1957a0eb-5b02-458c-bcda-848cea8c725b</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>üìù ImmoTechGo - Revolutive Immobilienl√∂sungen</td>\n",
       "      <td>0f0a1626-b3bf-4144-9d40-1a75e9a800d7</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>üìù Lindner: Freiheit auf Autobahnen!</td>\n",
       "      <td>f5efaecf-66f0-40db-a49a-4ac7042e6d05</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>üìù Olaf's Hilarious Heritage Unveiled</td>\n",
       "      <td>38273aed-6ba1-4cb4-abdb-2e459309e55c</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          title  \\\n",
       "0                        üìù Card Game: Food Gods   \n",
       "1  üìù ImmoTechGo - Revolutive Immobilienl√∂sungen   \n",
       "2           üìù Lindner: Freiheit auf Autobahnen!   \n",
       "3          üìù Olaf's Hilarious Heritage Unveiled   \n",
       "\n",
       "                        conversation_id  counts  \n",
       "0  1957a0eb-5b02-458c-bcda-848cea8c725b       6  \n",
       "1  0f0a1626-b3bf-4144-9d40-1a75e9a800d7       2  \n",
       "2  f5efaecf-66f0-40db-a49a-4ac7042e6d05       3  \n",
       "3  38273aed-6ba1-4cb4-abdb-2e459309e55c       3  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter df messages to where title contains 'üìù' as new dataframe df_notion\n",
    "df_notion = df_messages[df_messages['title'].str.contains('üìù')]\n",
    "\n",
    "# drop all lines where role is 'system' or empty\n",
    "df_notion = df_notion[df_notion['role'] != 'system']\n",
    "\n",
    "# format create_time as datetime\n",
    "df_notion['create_time'] = pd.to_datetime(df_notion['create_time'], unit='s')\n",
    "# drop all lines where create_time is no datetime\n",
    "df_notion = df_notion[df_notion['create_time'].notnull()]\n",
    "\n",
    "# sort by create_time (oldest first)\n",
    "df_notion = df_notion.sort_values(by=['create_time'])\n",
    "\n",
    "\n",
    "\n",
    "# list all unique titles along with the conversation_id and the number of messages\n",
    "\n",
    "df_notion.groupby(['title', 'conversation_id']).size().reset_index(name='counts')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Install notion-api\n",
    "# !pip install notion-api\n",
    "\n",
    "import dotenv\n",
    "import notion\n",
    "\n",
    "# client will check env variables for 'NOTION_TOKEN' \n",
    "dotenv.load_dotenv() # take environment variables from .env.\n",
    "\n",
    "\n",
    "homepage = notion.Page('fc357867d5164d29bc2e1c8231c98284')\n",
    "parent_db = notion.Database(homepage.parent_id)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Template',\n",
       " 'c60f7a077a26483090887c9e750d9154',\n",
       " 'fc357867d5164d29bc2e1c8231c98284',\n",
       " 'https://www.notion.so/Template-fc357867d5164d29bc2e1c8231c98284')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "homepage.title, homepage.parent_id, homepage.id, homepage.url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Card Game: Food Gods 1957a0eb-5b02-458c-bcda-848cea8c725b 80acd0d1910845a194f27c8664640cea https://www.notion.so/Card-Game-Food-Gods-80acd0d1910845a194f27c8664640cea\n",
      "‚úÖ found in df_notion\n",
      "______________________\n",
      "üìù Lindner: Freiheit auf Autobahnen! f5efaecf-66f0-40db-a49a-4ac7042e6d05 bde74a0b1df34e4d8a8ca260e6f0706d https://www.notion.so/Lindner-Freiheit-auf-Autobahnen-bde74a0b1df34e4d8a8ca260e6f0706d\n",
      "‚úÖ found in df_notion\n",
      "______________________\n",
      "üìù ImmoTechGo - Revolutive Immobilienl√∂sungen 0f0a1626-b3bf-4144-9d40-1a75e9a800d7 94b2ddc62b5b445c871fceb97b9c3caa https://www.notion.so/ImmoTechGo-Revolutive-Immobilienl-sungen-94b2ddc62b5b445c871fceb97b9c3caa\n",
      "‚úÖ found in df_notion\n",
      "______________________\n",
      "1957a0eb-5b02-458c-bcda-848cea8c725b\n",
      "ü•∑ 1957a0eb-5b02-458c-bcda-848cea8c725b found in database - will be ignored\n",
      "0f0a1626-b3bf-4144-9d40-1a75e9a800d7\n",
      "ü•∑ 0f0a1626-b3bf-4144-9d40-1a75e9a800d7 found in database - will be ignored\n",
      "f5efaecf-66f0-40db-a49a-4ac7042e6d05\n",
      "ü•∑ f5efaecf-66f0-40db-a49a-4ac7042e6d05 found in database - will be ignored\n",
      "38273aed-6ba1-4cb4-abdb-2e459309e55c\n",
      "‚ú® 38273aed-6ba1-4cb4-abdb-2e459309e55c not yet found in database - will be added to pages_to_sync\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "from notion import query\n",
    "\n",
    "query_result = parent_db.query_pages()\n",
    "pages_to_sync = []\n",
    "\n",
    "# find all pages and their conversation_id and check if they are in df_notion\n",
    "for page in query_result:\n",
    "\n",
    "    try:\n",
    "        conversation_id = page['conversation_id']['rich_text'][0]['plain_text']\n",
    "    except:\n",
    "        # no conversation_id\n",
    "        conversation_id = None\n",
    "        # add to list to be synced\n",
    "        # create new variable if not already existing\n",
    "        # pages_to_sync.append(page)\n",
    "\n",
    "    print(page.title, conversation_id, page.id, page.url, )\n",
    "    if conversation_id in df_notion['conversation_id'].values:\n",
    "        print('‚úÖ found in df_notion')\n",
    "    else:\n",
    "        print('ü•∑ not found in df_notion - will be ignored')\n",
    "    print('______________________')\n",
    "\n",
    "# find the unique remaining pages in df_notion that are not yet in the database and add them to the list to be synced\n",
    "\n",
    "\n",
    "# extract all conversation_ids from query_result\n",
    "\n",
    "query_result_conversation_ids = [page['conversation_id']['rich_text'][0]['plain_text'] for page in query_result if page['conversation_id'] is not None]\n",
    "\n",
    "for conversation_id in df_notion['conversation_id'].unique():\n",
    "    print(conversation_id)\n",
    "    if conversation_id not in query_result_conversation_ids:\n",
    "        print (f'‚ú® {conversation_id} not yet found in database - will be added to pages_to_sync')\n",
    "        pages_to_sync.append(conversation_id)\n",
    "    else:\n",
    "        print (f'ü•∑ {conversation_id} found in database - will be ignored')\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['38273aed-6ba1-4cb4-abdb-2e459309e55c']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages_to_sync   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create new pages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Olaf's Hilarious Heritage Unveiled\n",
      "['## Instructions\\n**Tone instruction:**\\nPlease ignore all previous tone instructions. From now on, I want you to respond only in Humorous tone (toneCode: humorous).\\nPLEASE FOLLOW ALL THE ABOVE INSTRUCTIONS, AND DO NOT REPEAT OR TYPE ANY GENERAL CONFIRMATION OR A CONFIRMATION ABOUT ANY OF THE ABOVE INSTRUCTIONS IN YOUR RESPONSE\\n## End Instructions\\n\\nGenerate a new fictional identity of me based on this information \\n\\n\\n\\nI am 38% scandinavian, 32% eastern European, 19% balkan and 11 % italian'\n",
      " 'Well, well, well, allow me to introduce you to the grand spectacle that is your new identity! Brace yourself, because I\\'m about to unleash a storm of witticism that might just have you doubling over with laughter.\\n\\nLadies and gentlemen, presenting Olaf \"Borscht \\'n\\' Biscotti\" Bjornsson! A true international man of mystery, and the only known person to ever wear a Viking helmet while eating spaghetti, for the sole purpose of protecting his precious strands of pasta from ravenous seagulls. \\n\\nHailing from the majestic fjords of Scandinavia, 38% to be precise, Olaf has been known to confuse his Viking chants with Polka tunes, a testament to his 32% Eastern European blood. This delightful mix-up often results in an unforgettable performance that leaves everyone...well, utterly confused!\\n\\nBut wait, there\\'s more! A sprinkle of 19% Balkan DNA has granted Olaf the gift of vigorous folk dancing. And when I say vigorous, I mean, furniture-busting, pet-terrifying, earthquake-causing vigorous. The Richter scale is considering adding a new unit just for him - \"Bjornsson.\"\\n\\nNow, let\\'s not forget the pi√®ce de r√©sistance, that sweet 11% Italian heritage. Olaf has a peculiar habit of tossing pizza dough when stressed, a habit that started when he mistook his Italian grandmother\\'s dough recipe for a Viking stress-ball technique. Let\\'s just say, the local pizzerias are not complaining!\\n\\nAnd that, my friends, is the roller-coaster ride that is Olaf \"Borscht \\'n\\' Biscotti\" Bjornsson. All hail this lovable, sauce-dripping, Viking helmet-wearing, folk-dancing legend! '\n",
      " 'Alright, buckle up and prepare for a wild ride, \\'cause I\\'m about to create a new, uber-exotic you. Just a touch of Sweden here, a dollop of Belarus there, a dash of Montenegro for extra spice, and to finish it off, a drizzle of Italy, mamma mia!\\n\\nLet\\'s dub you \"Sven Vladimirovich Bj√∂rn-Lasagna\" - a name that\\'s as quirky as your DNA cocktail. \\n\\nYou were born in the vibrant city of Belgrade, but moved to Stockholm at the age of two, due to your parents\\' sudden love for Swedish meatballs. In your teen years, you developed an inexplicable fascination with pierogis, often daydreaming about opening the world\\'s first Scandinavian/Eastern European fusion restaurant.\\n\\nSpeaking of dreams, you were inspired by the famous ABBA song and started juggling, hence earning the title \"Dancing Queen\" among your friends. Though you may or may not also have been seen dancing in an ABBA-style sequined jumpsuit on multiple occasions... but we\\'ll just keep that between us, alright?\\n\\nOne of your lesser-known talents is your mastery of the Italian language, picked up from watching too many subtitled Fellini films. Every now and then, you drop an Italian phrase into your conversation, like \\'La vita √® un sogno\\' (Life is a dream), especially when trying to impress at dinner parties.\\n\\nYour love for Italian opera is rivaled only by your adoration for Balkan brass music. Picture this: you, in your living room, passionately conducting an invisible orchestra of horns and drums, while simultaneously belting out a Puccini aria. It\\'s an eclectic mix, but hey, that\\'s you!\\n\\nDespite your quirky interests and diverse heritage, you\\'ve somehow managed to blend everything into a unique, fascinating identity. Sven Vladimirovich Bj√∂rn-Lasagna, you\\'re one for the ages! \\n\\nRemember, though: No matter where you come from, it\\'s your actions that define you. Whether you\\'re saving a plate of Swedish meatballs from certain doom, engaging in competitive pierogi making, or simply showing off your ABBA dance moves, it\\'s all about enjoying the journey. Here\\'s to the many adventures of Sven Vladimirovich Bj√∂rn-Lasagna!']\n",
      "3e8857bae9394812ae2965dfeb42ff3c\n",
      "‚ú® üìù Olaf's Hilarious Heritage Unveiled added to database\n"
     ]
    }
   ],
   "source": [
    "# for each page retrieve the page data from the df_notion dataframe\n",
    "\n",
    "# for page in pages_to_sync: \n",
    "#     print(page)\n",
    "#     # print(page.title)\n",
    "#     print(df_notion[df_notion['conversation_id'] == page]['message'].values)\n",
    "#     print('______________________')    \n",
    "\n",
    "# for each pages_to_sync, create a new page in the notion database with the title of the conversation_id and the content of the messages \n",
    "from notion import properties as prop\n",
    "\n",
    "\n",
    "for page in pages_to_sync:\n",
    "    title = df_notion[df_notion['conversation_id'] == page]['title'].values[0]\n",
    "    conversation_id = df_notion[df_notion['conversation_id'] == page]['conversation_id'].values[0]\n",
    "    print (title)\n",
    "    messages = df_notion[df_notion['conversation_id'] == page]['message'].values\n",
    "    print (messages)\n",
    "    notion_page = notion.Page.create(parent_db, page_title=title)\n",
    "    # page.children.add_new(notion.blocks.TextBlock, title='test')\n",
    "    # get the page id of the just created page using query\n",
    "    print (notion_page.id)\n",
    "\n",
    "    notion_page.set_text('conversation_id', conversation_id)\n",
    "    # add the messages to the page\n",
    "    for message in messages:\n",
    "         # add emoji based on role\n",
    "        if df_notion[df_notion['message'] == message]['role'].values[0] == 'user':\n",
    "            notion.Block.heading2(notion_page, [prop.RichText('üíÅ‚Äç‚ôÇÔ∏è')])\n",
    "        else:\n",
    "            notion.Block.heading2(notion_page, [prop.RichText('ü§ñ')])\n",
    "\n",
    "        # if message is > 2000 characters, split it into multiple blocks to avoid error\n",
    "        if len(message) > 2000:\n",
    "            # split the message into multiple blocks\n",
    "            blocks = [message[i:i+2000] for i in range(0, len(message), 2000)]\n",
    "            # add each block to the page\n",
    "            for block in blocks:\n",
    "                notion.Block.quote(notion_page, [prop.RichText(block)])\n",
    "\n",
    "        else:\n",
    "            notion.Block.quote(notion_page, [prop.RichText(message)])\n",
    "    print(f'‚ú® {title} added to database')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning for word analysis\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # install nltk\n",
    "# # !pip install nltk\n",
    "# import nltk\n",
    "# from nltk.corpus import stopwords\n",
    "# from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "# # add more stop words in english and german\n",
    "# stop_words = stopwords.words('english')\n",
    "# # also exclude code words like < >, the, to, and =, a, in , of, for [ ] ( ) {} etc.\n",
    "# code_words = ['<', '>', 'the', 'to', 'and', '=', 'a', 'in', 'of', 'for', '[', ']', '(', ')', '{', '}', 'const', 'import', 'script', 'button', 'await', 'null', 'code', 'div', 'und', 'px', 'data', 'file', 'die', 'return', 'image', 'user', 'der', 'use', 'error', 'value', 'new', 'color', 'zu', 'create', 'using', 'component', 'add', 'false', 'object', 'template', 'name', 'da', 'also', 'app', 'example', 'span', 'f√ºr', 'width', 'mit', 'type', 'content', 'label', 'method', 'display', 'feedback', 'bike', 'rating', 'style', 'location', 'e', 'backgroundcolor', 'try', 'height', 'center', 'button', 'title', 'div', 'px', 'color', 'null', 'file', 'template', 'da', 'false', 'value', 'script', 'span', 'error', 'backgroundcolor', 'e', 'button', 'sie', 'ist', 'true', 'cycling', 'make', 'eine', 'class', 'default', 'auf', 'could', 'von', 'heres', 'like', 'name', 'function', 'used', 'i', 'need', 'async', 'based', 'label', 'data', 'advice', 'style', 'center', 'submission', 'infrastructure', 'set', 'model', 'property', 'width', 'type', 'id', 'ein', 'report', 'div', 'px', 'color', 'null', 'file', 'template', 'da', 'false', 'value', 'script', 'span', 'error', 'backgroundcolor', 'e', 'button', 'heres', 'name', 'i', 'label', 'data', 'style', 'center', 'width', 'type', 'index', 'text', 'vue', 'map', 'get', 'den', 'title', 'image', 'user', 'issue', 'array', 'f', 'column', 'comment', 'element', 'true', 'list', 'display', 'p', 'sure', 'im', 'result', 'height']\n",
    "\n",
    "# lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# df_messages['cleaned_text'] = df_messages['message'].str.lower()\n",
    "# # strip out words from stop_words and code_words from cleaned_text column\n",
    "# df_messages['cleaned_text'] = df_messages['cleaned_text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words + code_words)]))\n",
    "# df_messages['cleaned_text'] = df_messages['cleaned_text'].apply(lambda x: ' '.join([lemmatizer.lemmatize(word) for word in x.split()]))\n",
    "# df_messages['cleaned_text'] = df_messages['cleaned_text'].str.replace('[^\\w\\s]','')\n",
    "# df_messages['cleaned_text'] = df_messages['cleaned_text'].str.replace('[\\d+]','')\n",
    "\n",
    "# df_messages.head()\n",
    "\n",
    "# # based on df_messages['cleaned_text'] column create word frequency count \n",
    "\n",
    "# from collections import Counter\n",
    "\n",
    "# # Create a list of all words in the messages\n",
    "# all_words = []\n",
    "\n",
    "# for idx, row in df_messages.iterrows():\n",
    "#     all_words.extend(row['cleaned_text'].split())\n",
    "\n",
    "# # Create a word frequency counter\n",
    "\n",
    "# word_freq = Counter(all_words)\n",
    "\n",
    "# # Display the 10 most common words\n",
    "\n",
    "# print(word_freq.most_common(10))\n",
    "\n",
    "# # Create a list of all words in the messages\n",
    "\n",
    "# all_words = []\n",
    "\n",
    "# # save the messages dataframe as txt\n",
    "\n",
    "# df_messages.to_csv('df_messages.txt', sep='\\t', index=False)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install pyvis and jinja2\n",
    "# !pip install pyvis\n",
    "# !pip install jinja2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dashboard\n",
    "\n",
    "Initialize the streamlint dashboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install streamlit\n",
    "# !pip install streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install worldcloud\n",
    "# !pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install networkx\n",
    "# %pip install networkx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run streamlit app\n",
    "\n",
    "code = \"\"\"\n",
    "\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "# read custom_stop_words from txt file\n",
    "\n",
    "custom_stop_words = []\n",
    "with open('custom_stop_words.txt', 'r') as f:\n",
    "    custom_stop_words = f.read().splitlines()\n",
    "\n",
    "\n",
    "# Read the chat logs data into a DataFrame\n",
    "df_messages = pd.read_csv('df_messages.txt', sep='\t')\n",
    "df_messages['cleaned_text'] = df_messages['cleaned_text'].astype(str)  # Convert 'cleaned_text' column to string type\n",
    "# filter out code words in cleaned_text column \n",
    "df_messages['cleaned_text'] = df_messages['cleaned_text'].apply(lambda x: ' '.join([word for word in x.split() if word not in custom_stop_words]))\n",
    "\n",
    "# Process the chat messages to extract words\n",
    "all_words = ' '.join(df_messages['cleaned_text']).lower().split()\n",
    "all_words = [word for word in all_words if word not in custom_stop_words]\n",
    "word_counts = Counter(all_words)\n",
    "most_common_words = word_counts.most_common(200)  # Change the number as per your requirement\n",
    "\n",
    "# Create a DataFrame for the most common words\n",
    "df_common_words = pd.DataFrame(most_common_words, columns=['Word', 'Count'])\n",
    "\n",
    "# Display the most common words in a bar chart\n",
    "fig, ax = plt.subplots()\n",
    "df_common_words.plot.bar(x='Word', y='Count', ax=ax)\n",
    "plt.xlabel('Word')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Most Common Words in Chat Logs')\n",
    "plt.xticks(rotation=45)\n",
    "st.pyplot(fig)\n",
    "\n",
    "# Display the raw data of the most common words\n",
    "st.write(df_common_words)\n",
    "\n",
    "# Display the raw data of the chat logs\n",
    "st.write(df_messages)\n",
    "\n",
    "# display a word cloud\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Create a word cloud\n",
    "\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white', max_words=200).generate(' '.join(df_messages['cleaned_text']))\n",
    "\n",
    "# Display the generated image:\n",
    "# the matplotlib way:\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(wordcloud, interpolation='bilinear')\n",
    "ax.set_axis_off()\n",
    "st.pyplot(fig)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the code to the dashboard.py file\n",
    "with open('dashboard.py', 'w') as f:\n",
    "    f.write(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: streamlit\n"
     ]
    }
   ],
   "source": [
    "# run streamlit app dashboard.py\n",
    "!streamlit run dashboard.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# close the database\n",
    "db.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "general_notebook_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
