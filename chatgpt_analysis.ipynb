{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install packages\n",
    "# !conda install -c conda-forge plyvel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# ChatGPT local analysis in Jupyter Notebook ðŸ¤–\n",
       "\n",
       "In this notebook I deliver a foundation on how to analyse the personal ChatGPT conversation history. \n",
       "\n",
       "After a few months of using ChatGPT, I have collected a large amount of data and want to reflect on old conversations and spark ideas on how to use it in the future.\n",
       "\n",
       "Using the Superpower ChatGPT extension for Chrome you can automatically sync your conversations for offline usage. Since all your conversations are now stored locally, you can analyse the local database to get insights into your conversations.\n",
       "\n",
       "## Demo\n",
       "Feel free to check out the notebook [here](./chatgpt_analysis.ipynb): \n",
       "## Requirements\n",
       "\n",
       "- Superpower ChatGPT extension: https://github.com/saeedezzati/superpower-chatgpt\n",
       "- Python 3.8\n",
       "- Streamlit\n",
       "\n",
       "\n",
       "## Current state\n",
       "\n",
       "- [x] Get data from local database to a pandas dataframe\n",
       "    - [x] df_conversations\n",
       "    - [x] df_messages\n",
       "\n",
       "- [x] Streamlint Dashboard\n",
       "    - [x] Basic setup\n",
       "    - [x] Table with conversations\n",
       "    - [x] Wordcloud of all messages\n",
       "    - [x] Exclusion list in custom_stop_words.txt\n",
       "    \n",
       "    - [ ] Conversation overview\n",
       "    - [ ] Graph network of conversations / words / topics\n",
       "\n",
       "## Future features\n",
       "\n",
       "- Graph network of conversations / words / topics (Can someone help me with this?) \n",
       "    - Not sure how to do this, but I think it would be cool to see how conversations are connected and how topics are connected to each other\n",
       "    - https://towardsdatascience.com/how-to-deploy-interactive-pyvis-network-graphs-on-streamlit-6c401d4c99db\n",
       "\n",
       "## Pull requests are welcome!\n",
       "Feel free to contribute to this project and contribute ideas on how to analyse the data."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import readme.md file (project overview) and display the markdown file here\n",
    "\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "with open('README.md', 'r') as f:\n",
    "    content = f.read()\n",
    "\n",
    "display(Markdown(content))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plyvel\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data from browser database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 75656\n",
      "-rw-------@ 1 phil  staff   7720330 May 30 17:41 003033.ldb\n",
      "-rw-------@ 1 phil  staff    131157 Jun  9 16:46 006828.ldb\n",
      "-rw-------@ 1 phil  staff    127701 Jun 10 09:25 007043.ldb\n",
      "-rw-------@ 1 phil  staff  22356554 Jun 10 09:26 007050.log\n",
      "-rw-------@ 1 phil  staff   7916084 Jun 10 09:26 007052.ldb\n",
      "-rw-------@ 1 phil  staff        16 May 28 21:05 CURRENT\n",
      "-rw-------@ 1 phil  staff         0 May 28 21:05 LOCK\n",
      "-rw-------@ 1 phil  staff     70166 Jun 10 09:26 LOG\n",
      "-rw-------@ 1 phil  staff      1774 Jun  8 18:10 LOG.old\n",
      "-rw-------@ 1 phil  staff    320509 Jun 10 09:26 MANIFEST-000001\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Path to the leveldb directory (change this to your path - in my case I am using Brave Browser)\n",
    "leveldb_path = '~/Library/Application Support/BraveSoftware/Brave-Browser/Default/Local Extension Settings/amhmeenmapldpjdedekalnfifgnpfnkc'\n",
    "leveldb_path = os.path.expanduser(leveldb_path)  # Expand the '~' symbol to the user's home directory\n",
    "\n",
    "# Enclose the path in quotes to handle spaces\n",
    "leveldb_path = f'\"{leveldb_path}\"'\n",
    "\n",
    "# Get the list of files in the leveldb directory\n",
    "\n",
    "!ls -l $leveldb_path\n",
    "\n",
    "# Copy all files to the current directory /db\n",
    "\n",
    "!cp -r $leveldb_path ./db\n",
    "\n",
    "# set leveldb path to the copied directory\n",
    "\n",
    "leveldb_path = './db/amhmeenmapldpjdedekalnfifgnpfnkc'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initiate Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'                               Compactions\\nLevel  Files Size(MB) Time(sec) Read(MB) Write(MB)\\n--------------------------------------------------\\n  0        1        7         0        0         7\\n  1        2        8         0        0         0\\n  2        2        7         0        0         0\\n'\n"
     ]
    }
   ],
   "source": [
    "db = plyvel.DB(leveldb_path)\n",
    "\n",
    "# get database info\n",
    "\n",
    "print(db.get_property(b'leveldb.stats'))\n",
    "\n",
    "\n",
    "# filter all key entries that contain chat.openai.com \n",
    "\n",
    "for key, value in db:\n",
    "    if b'chat.openai.com' in key:\n",
    "        print(f'Key: {key}, Value: {value}')\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.DataFrame(columns=['key', 'value'])\n",
    "\n",
    "for key, value in db:\n",
    "    df = pd.concat([df, pd.DataFrame({'key': [key], 'value': [value]})], ignore_index=True)\n",
    "    # df = df.append({'key': key, 'value': value}, ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the value of key \"conversations\"  as json in a separate dataframe\n",
    "\n",
    "df_conversations = pd.DataFrame(json.loads(df[df['key'] == b'conversations']['value'].values[0]))\n",
    "# with column / lines transposed\n",
    "df_conversations = df_conversations.T\n",
    "# also export as text file\n",
    "df_conversations.to_csv('df_conversations.txt', sep='\\t', index=False)\n",
    "\n",
    "# # add a column for the key value length\n",
    "df['key_length'] = df['value'].str.len()\n",
    "\n",
    "df.head()\n",
    "\n",
    "# close the database\n",
    "db.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     id                       conversation_id  \\\n",
      "0  430741cb-bd4b-44bd-b38a-fffeb45a3b49  000250bf-4b4b-47de-9bb9-2425ebccf8c0   \n",
      "1  72ed73e5-3856-4b72-9700-23dc8acd0015  000250bf-4b4b-47de-9bb9-2425ebccf8c0   \n",
      "2  af30cc11-ce74-475a-8a21-aa9d375ea1e7  000250bf-4b4b-47de-9bb9-2425ebccf8c0   \n",
      "3  eea89b97-3e61-424c-9f92-c57d7706973f  000250bf-4b4b-47de-9bb9-2425ebccf8c0   \n",
      "4  f248e179-f8fd-4184-8163-20a44ab381fa  000250bf-4b4b-47de-9bb9-2425ebccf8c0   \n",
      "\n",
      "      title                                            message       role  \\\n",
      "0  New chat  Any ideas for a cycling-related data-driven gr...       user   \n",
      "1  New chat                                                        system   \n",
      "2  New chat  Any ideas for a data-driven graduation project...       user   \n",
      "3  New chat  Here are a few ideas for a cycling-related dat...  assistant   \n",
      "4  New chat  Here are a few ideas for a data-driven graduat...  assistant   \n",
      "\n",
      "         create_time  \n",
      "0  1671022256.202531  \n",
      "1  1671021508.989812  \n",
      "2  1671021508.990174  \n",
      "3  1671022299.506987  \n",
      "4  1671021555.109059  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# drop all rows of df_conversations with empty mapping\n",
    "df_conversations = df_conversations[df_conversations['mapping'].str.len() > 0]\n",
    "\n",
    "# Create a new DataFrame for messages\n",
    "df_messages = pd.DataFrame(columns=['id','conversation_id', 'title', 'message', 'role', 'create_time'])\n",
    "\n",
    "# Iterate over the conversations and messages\n",
    "for idx, conversation in df_conversations.iterrows():\n",
    "    mapping = conversation['mapping']\n",
    "    title = conversation['title']\n",
    "    conversation_id = conversation['id']\n",
    "    \n",
    "    # Iterate over the messages in the mapping\n",
    "    for message_id, message_data in mapping.items():\n",
    "        if 'message' in message_data and message_data['message'] is not None:\n",
    "            message = message_data['message']\n",
    "            \n",
    "            # Get the message text\n",
    "            message_text = ''\n",
    "            if 'content' in message and 'parts' in message['content']:\n",
    "                message_parts = message['content']['parts']\n",
    "                if len(message_parts) > 0:\n",
    "                    message_text = message_parts[0]\n",
    "            \n",
    "            # Get the author role\n",
    "            role = ''\n",
    "            if 'author' in message and 'role' in message['author']:\n",
    "                role = message['author']['role']\n",
    "            \n",
    "            # Get the create time\n",
    "            create_time = ''\n",
    "            if 'create_time' in message:\n",
    "                create_time = message['create_time']\n",
    "            \n",
    "            # Append the message data to the DataFrame\n",
    "            # df_messages = df_messages.append({\n",
    "            #     'id': message_id,\n",
    "            #     'title': title,\n",
    "            #     'message': message_text,\n",
    "            #     'role': role,\n",
    "            #     'create_time': create_time\n",
    "            # }, ignore_index=True)\n",
    "\n",
    "            df_messages = pd.concat([df_messages, pd.DataFrame({\n",
    "                'id': [message_id],\n",
    "                'conversation_id': [conversation_id],\n",
    "                'title': [title],\n",
    "                'message': [message_text],\n",
    "                'role': [role],\n",
    "                'create_time': [create_time]\n",
    "                })], ignore_index=True)\n",
    "            \n",
    "\n",
    "\n",
    "# Close the LevelDB database\n",
    "db.close()\n",
    "\n",
    "# Display the first few rows of the messages DataFrame\n",
    "print(df_messages.head())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/phil/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/phil/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "/var/folders/wc/1fgpcwpx4rzfbbc7shb_wy8h0000gn/T/ipykernel_57513/1433106567.py:19: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df_messages['cleaned_text'] = df_messages['cleaned_text'].str.replace('[^\\w\\s]','')\n",
      "/var/folders/wc/1fgpcwpx4rzfbbc7shb_wy8h0000gn/T/ipykernel_57513/1433106567.py:20: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df_messages['cleaned_text'] = df_messages['cleaned_text'].str.replace('[\\d+]','')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('div', 11351), ('px', 7869), ('color', 3109), ('null', 2996), ('file', 2651), ('template', 2567), ('da', 2432), ('false', 2331), ('value', 2319), ('script', 2258)]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# add more stop words in english and german\n",
    "stop_words = stopwords.words('english')\n",
    "# also exclude code words like < >, the, to, and =, a, in , of, for [ ] ( ) {} etc.\n",
    "code_words = ['<', '>', 'the', 'to', 'and', '=', 'a', 'in', 'of', 'for', '[', ']', '(', ')', '{', '}', 'const', 'import', 'script', 'button', 'await', 'null', 'code', 'div', 'und', 'px', 'data', 'file', 'die', 'return', 'image', 'user', 'der', 'use', 'error', 'value', 'new', 'color', 'zu', 'create', 'using', 'component', 'add', 'false', 'object', 'template', 'name', 'da', 'also', 'app', 'example', 'span', 'fÃ¼r', 'width', 'mit', 'type', 'content', 'label', 'method', 'display', 'feedback', 'bike', 'rating', 'style', 'location', 'e', 'backgroundcolor', 'try', 'height', 'center', 'button', 'title', 'div', 'px', 'color', 'null', 'file', 'template', 'da', 'false', 'value', 'script', 'span', 'error', 'backgroundcolor', 'e', 'button', 'sie', 'ist', 'true', 'cycling', 'make', 'eine', 'class', 'default', 'auf', 'could', 'von', 'heres', 'like', 'name', 'function', 'used', 'i', 'need', 'async', 'based', 'label', 'data', 'advice', 'style', 'center', 'submission', 'infrastructure', 'set', 'model', 'property', 'width', 'type', 'id', 'ein', 'report', 'div', 'px', 'color', 'null', 'file', 'template', 'da', 'false', 'value', 'script', 'span', 'error', 'backgroundcolor', 'e', 'button', 'heres', 'name', 'i', 'label', 'data', 'style', 'center', 'width', 'type', 'index', 'text', 'vue', 'map', 'get', 'den', 'title', 'image', 'user', 'issue', 'array', 'f', 'column', 'comment', 'element', 'true', 'list', 'display', 'p', 'sure', 'im', 'result', 'height']\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "df_messages['cleaned_text'] = df_messages['message'].str.lower()\n",
    "# strip out words from stop_words and code_words from cleaned_text column\n",
    "df_messages['cleaned_text'] = df_messages['cleaned_text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words + code_words)]))\n",
    "df_messages['cleaned_text'] = df_messages['cleaned_text'].apply(lambda x: ' '.join([lemmatizer.lemmatize(word) for word in x.split()]))\n",
    "df_messages['cleaned_text'] = df_messages['cleaned_text'].str.replace('[^\\w\\s]','')\n",
    "df_messages['cleaned_text'] = df_messages['cleaned_text'].str.replace('[\\d+]','')\n",
    "\n",
    "df_messages.head()\n",
    "\n",
    "# based on df_messages['cleaned_text'] column create word frequency count \n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# Create a list of all words in the messages\n",
    "all_words = []\n",
    "\n",
    "for idx, row in df_messages.iterrows():\n",
    "    all_words.extend(row['cleaned_text'].split())\n",
    "\n",
    "# Create a word frequency counter\n",
    "\n",
    "word_freq = Counter(all_words)\n",
    "\n",
    "# Display the 10 most common words\n",
    "\n",
    "print(word_freq.most_common(10))\n",
    "\n",
    "# Create a list of all words in the messages\n",
    "\n",
    "all_words = []\n",
    "\n",
    "# save the messages dataframe as txt\n",
    "\n",
    "df_messages.to_csv('df_messages.txt', sep='\\t', index=False)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install pyvis and jinja2\n",
    "# !pip install pyvis\n",
    "# !pip install jinja2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dashboard\n",
    "\n",
    "Initialize the streamlint dashboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install streamlit\n",
    "# !pip install streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install worldcloud\n",
    "# !pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install networkx\n",
    "# %pip install networkx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run streamlit app\n",
    "\n",
    "code = \"\"\"\n",
    "\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "# read custom_stop_words from txt file\n",
    "\n",
    "custom_stop_words = []\n",
    "with open('custom_stop_words.txt', 'r') as f:\n",
    "    custom_stop_words = f.read().splitlines()\n",
    "\n",
    "\n",
    "# Read the chat logs data into a DataFrame\n",
    "df_messages = pd.read_csv('df_messages.txt', sep='\t')\n",
    "df_messages['cleaned_text'] = df_messages['cleaned_text'].astype(str)  # Convert 'cleaned_text' column to string type\n",
    "# filter out code words in cleaned_text column \n",
    "df_messages['cleaned_text'] = df_messages['cleaned_text'].apply(lambda x: ' '.join([word for word in x.split() if word not in custom_stop_words]))\n",
    "\n",
    "# Process the chat messages to extract words\n",
    "all_words = ' '.join(df_messages['cleaned_text']).lower().split()\n",
    "all_words = [word for word in all_words if word not in custom_stop_words]\n",
    "word_counts = Counter(all_words)\n",
    "most_common_words = word_counts.most_common(200)  # Change the number as per your requirement\n",
    "\n",
    "# Create a DataFrame for the most common words\n",
    "df_common_words = pd.DataFrame(most_common_words, columns=['Word', 'Count'])\n",
    "\n",
    "# Display the most common words in a bar chart\n",
    "fig, ax = plt.subplots()\n",
    "df_common_words.plot.bar(x='Word', y='Count', ax=ax)\n",
    "plt.xlabel('Word')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Most Common Words in Chat Logs')\n",
    "plt.xticks(rotation=45)\n",
    "st.pyplot(fig)\n",
    "\n",
    "# Display the raw data of the most common words\n",
    "st.write(df_common_words)\n",
    "\n",
    "# Display the raw data of the chat logs\n",
    "st.write(df_messages)\n",
    "\n",
    "# display a word cloud\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Create a word cloud\n",
    "\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white', max_words=200).generate(' '.join(df_messages['cleaned_text']))\n",
    "\n",
    "# Display the generated image:\n",
    "# the matplotlib way:\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(wordcloud, interpolation='bilinear')\n",
    "ax.set_axis_off()\n",
    "st.pyplot(fig)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the code to the dashboard.py file\n",
    "with open('dashboard.py', 'w') as f:\n",
    "    f.write(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\n",
      "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
      "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.24.144.160:8501\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[34m\u001b[1m  For better performance, install the Watchdog module:\u001b[0m\n",
      "\n",
      "  $ xcode-select --install\n",
      "  $ pip install watchdog\n",
      "            \u001b[0m\n",
      "^C\n",
      "\u001b[34m  Stopping...\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# run streamlit app dashboard.py\n",
    "!streamlit run dashboard.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# close the database\n",
    "db.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "general_notebook_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
